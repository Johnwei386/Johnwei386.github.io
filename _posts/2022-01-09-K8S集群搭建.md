---
layout:     post
title:      K8S集群搭建
subtitle:   在Ubuntu18.04上的virtualbox虚拟机搭建K8s集群
date:       2022-01-09
author:     Johnwei386
header-img: img/post-bg-rwd.jpg
catalog: true
tags:
    - Docker
    - Kubernetes
---

## 1. 虚拟机安装与配置

物理机操作系统是Ubuntu18.04，安装有Virtualbox虚拟化软件；

在VirtualBox安装虚拟机，虚拟机镜像采用[debian-9.13.0-amd64-xfce-CD-1.iso](https://img.cs.montana.edu/linux/debian/9/amd64/debian-9.13.0-amd64-xfce-CD-1.iso)镜像，配置情况如下：

> 1. 三台Debian9虚拟机；
> 2. 每台机器至少2GB以上内存；
> 3. 用作控制平台节点的虚拟机至少要2核以上；
> 4. 集群中所有计算机之间必须具有完全的网络连接；

在Virtualbox上安装debian9，安装过程(略)，注意在设置系统分区时，需要手动分区，然后不分配swap分区；

设置一个app用户，密码默认为app；

虚拟机的网络设置为仅主机(Host-Only)网络，禁用该网络的DHCP功能，等系统创建成功后手动设置静态IP地址；

安装debian系统后进行如下操作：

1. 配置网络

   ```bash
   # 编辑/etc/network/interfaces文件, 配置静态ip
   auto enp0s3
   iface enp0s3  inet static
       address 192.168.56.12
       netmask 255.255.255.0
       gateway 192.168.56.1
       dns-nameservers 114.114.114.114
    
   # 编辑/etc/resolv.conf文件, 设置dns服务器, 若文件不存在则新建之
   nameserver  114.114.114.114
   
   # 重启网络服务
   service  networking  restart
   
   # 查看enp0s3网卡是否分配了ip地址
   ip -c addr show
   
   # ping一下网关,看通不通
   
   # 在运行virtualbox软件的物理机执行下面指令, 其中eth0是可以连通外网的网卡
   sudo iptables -F
   sudo iptables -P INPUT ACCEPT
   sudo iptables -P FORWARD ACCEPT
   sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE  
   
   # 在虚拟机ping一下外网地址,看是否可以连上外网
   ping  baidu.com
   ```

2. 配置Debian9国内源

   ```bash
   # 编辑/etc/apt/sources.list文件
   # deb http://security.debian.org/ stretch/updates main contrib non-free
   # deb-src http://security.debian.org/ stretch/updates main contrib non-free
   deb http://ftp.cn.debian.org/debian/ stretch main contrib non-free
   deb-src http://ftp.cn.debian.org/debian/ stretch main contrib non-free
   deb http://ftp.cn.debian.org/debian/ stretch-updates main contrib non-free
   deb-src http://ftp.cn.debian.org/debian/ stretch-updates main contrib non-free
   
   
   # 更新源
   apt-get  update
   
   # 安装ssh
   apt-get  install ssh
   
   ```

3. 配置app用户sudo权限

   ```bash
   # 系统中没有sudo, 则安装sudo
   apt-get install sudo
   
   # 添加app用户到sudo用户组
   usermod  -aG  sudo  app
   
   # 添加配置文件到/etc/sudoers.d/目录
   # 这个是系统文档推荐的做法，/etc/sudoers.d/ 目录中的文件相当于是 etc/sudoers 文件的补充。
   # 如果你写的配置文件有问题或者是想去除用户的 sudo 权限，直接删除文件即可，
   # 不用去修改 /etc/sudoers 文件，不会影响到系统默认配置。
   tee /etc/sudoers.d/app  <<<  'app ALL=(ALL) ALL'
   tee /etc/sudoers.d/app  <<<  'app ALL=(ALL) NOPASSWD: ALL'   # 设置每次执行 sudo 免密,即不输入密码执行sudo
   
   ```

4. 安装docker, 安装教程参考[docker-ce](https://docs.docker.com/engine/install/debian/)

   ```bash
   # 安装依赖
   sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
      
   # 添加官方源秘钥
   curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
   
   # 添加官方源
    echo \
     "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \
     $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   
   # 安装docker
   sudo apt-get update
   sudo apt-get install docker-ce
   
   # 添加app到docker用户组
   sudo usermod -aG docker ${USER}
   ```

5. 编辑/etc/hosts文件

   ```bash
   192.168.56.11    K8sNode1
   192.168.56.12    K8sMaster    # 控制节点
   192.168.56.13    K8sNode2
   ```

6. 配置iptables可以查看桥接流量

   ```bash
   # 检查br_netfilter 模块是否被加载
   lsmod | grep br_netfilter 
   
   # 显式加载该模块，重启后失效
   sudo modprobe br_netfilter
   
   # 持久化配置，内核自动加载br_netfilter模块
   cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
   br_netfilter
   EOF
   
   # 持久化配置，使iptables可以查看桥接流量
   cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   EOF
   
   # sysctl用于在内核运行时动态地修改内核的运行参数
   sudo sysctl --system
   ```

7. 安装 kubeadm、kubelet 和 kubectl, 三者的版本要一致

   - `kubeadm`：用来初始化集群的指令；
   - `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等；
   - `kubectl`：用来与集群通信的命令行工具；

   ```bash
   # 更新apt包索引并安装相关依赖
   sudo apt-get update
   sudo apt-get install -y apt-transport-https ca-certificates curl
   
   # 下载并添加清华源签名密钥, 但该秘钥已不可用于清华镜像库,需要设置apt不检验清华镜像库的证书
   curl -s https://gitee.com/thepoy/k8s/raw/master/apt-key.gpg | sudo apt-key add -
   
   # 添加 Kubernetes的apt源仓库，这里使用清华源镜像
   cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 
   deb https://mirrors.tuna.tsinghua.edu.cn/kubernetes/apt kubernetes-xenial main
   EOF
   
   # 设置apt不对清华镜像的证书进行验证
   cat <<EOF | sudo tee /etc/apt/apt.conf.d/99influxdata-cert
   Acquire::https::mirrors.tuna.tsinghua.edu.cn::Verify-Peer "false";
   EOF
   
   # 更新apt包索引，然后安装kubelet、kubeadm 和 kubectl，并锁定其版本
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl  # 锁定版本,不轻易进行升级
   ```

以上便完成了虚拟机的相关安装工作，关闭该虚拟机，然后重复上述操作再建另外两台虚拟机。

## 2. 创建集群

#### 2.1 查看kubeadm初始化所需镜像

kubeadm 初始化时会拉取一些 docker 镜像，镜像列表如下：

```bash
app@K8sMaster:~$ kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.23.1
k8s.gcr.io/kube-controller-manager:v1.23.1
k8s.gcr.io/kube-scheduler:v1.23.1
k8s.gcr.io/kube-proxy:v1.23.1
k8s.gcr.io/pause:3.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
```

可以看到，使用的镜像仓库是国内无法正常访问的`k8s.gcr.io`，所以需要另想办法完成镜像拉取；

**主节点**(控制节点)需要拉取**所有镜像**，**子节点**(从节点)只需要**`pause`和`kube-proxy`两个镜像**就能正常工作。

#### 2.2 配置Docker使用socks5代理

1. 创建docker服务插件目录

   ```bash
   sudo mkdir -p /etc/systemd/system/docker.service.d
   ```

2. 创建一个名为http-proxy.conf的文件

   ```bash
   sudo touch /etc/systemd/system/docker.service.d/http-proxy.conf
   ```

3. 编辑http-proxy.conf的文件

   ```bash
   sudo vim /etc/systemd/system/docker.service.d/http-proxy.conf
   
   # 写入内容
   [Service]
   Environment="HTTP_PROXY=socks5://127.0.0.1:1080" "HTTPS_PROXY=socks5://127.0.0.1:1080" "NO_PROXY=localhost,127.0.0.1,docker-registry.somecorporation.com"
   ```

4. 重新加载服务程序的配置文件

   ```bash
   sudo systemctl daemon-reload
   ```

5. 重启docker

   ```bash
   sudo systemctl restart docker
   ```

6. 验证是否配置成功

   ```bash
   systemctl show --property=Environment docker
   ```

#### 2.3 拉取镜像

使用docker依次拉取下面的镜像：

```bash
docker pull k8s.gcr.io/kube-apiserver:v1.23.1
docker pull k8s.gcr.io/kube-controller-manager:v1.23.1
docker pull k8s.gcr.io/kube-scheduler:v1.23.1
docker pull k8s.gcr.io/kube-proxy:v1.23.1
docker pull k8s.gcr.io/pause:3.6
docker pull k8s.gcr.io/etcd:3.5.1-0
docker pull k8s.gcr.io/coredns/coredns:v1.8.6
```

保存镜像到本地，用来迁移镜像

```bash
docker save -o kube-apiserver.tar k8s.gcr.io/kube-apiserver
docker save -o kube-apiserver.tar k8s.gcr.io/kube-apiserver:v1.23.1
docker save -o kube-proxy.tar k8s.gcr.io/kube-proxy:v1.23.1
docker save -o kube-controller-manager.tar k8s.gcr.io/kube-controller-manager:v1.23.1
docker save -o kube-scheduler.tar k8s.gcr.io/kube-scheduler:v1.23.1
docker save -o etcd.tar k8s.gcr.io/etcd:3.5.1-0
docker save -o coredns.tar k8s.gcr.io/coredns/coredns:v1.8.6
docker save -o pause.tar k8s.gcr.io/pause:3.6
```

载入docker镜像

```bash
# K8sMaster装载如下镜像
docker load --input coredns.tar
docker load --input kube-apiserver.tar
docker load --input etcd.tar
docker load --input kube-controller-manager.tar
docker load --input kube-proxy.tar
docker load --input kube-scheduler.tar
docker load --input pause.tar
docker images  # 列出所有镜像

# K8sNode1 和 K8sNode2装载如下镜像
docker load --input pause.tar
docker load --input kube-proxy.tar
docker images 
```

#### 2.4 初始化kubeadm

**kubeadm**需要使用 root 账户或以 root 权限运行，同时，kubeadm 需要使用 systemd 来管理容器的 cgroup；

在初始化前，修改 docker 的 daemon.json文件，daemon.json是docker的默认配置文件，默认路径为/etc/docker/daemon.json，但它一般不存在，需要手动创建daemon.json文件，然后添加一行`"exec-opts": ["native.cgroupdriver=systemd"]`，该操作需要在所有节点执行。

```bash
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}
```

重启docker服务

```bash
sudo systemctl daemon-reload
sudo systemctl restart docker
```

执行kubeadm初始化命令, 

```bash
sudo kubeadm init \
  --apiserver-advertise-address=192.168.56.12 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16
```

- 第一个参数是主节点的 ip 地址
- 第二个参数是为 service 另指定一个 ip 地址段
- 第三个参数是为 pod 网络指定的 ip 地址段

初始化成功后，最后会输出类似下面的结果：

```bash
[init] Using Kubernetes version: v1.23.1
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: missing optional cgroups: hugetlb
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8smaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.12]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8smaster localhost] and IPs [192.168.56.12 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8smaster localhost] and IPs [192.168.56.12 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 5.009904 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8smaster as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8smaster as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 2vdmjf.dpkodac4gz1oydtw
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.56.12:6443 --token 2vdmjf.dpkodac4gz1oydtw \
	--discovery-token-ca-cert-hash  sha256:8d25f39ffe032f8c0e3456b7ea8861292ec799a059d96d17863b501161bb6bbd 
```

若中途初始化失败，则需要执行如下命令重置k8s集群状态

```bash
sudo systemctl restart docker.service
sudo kubeadm reset
sudo rm -rf /var/lib/etcd  # 如果该文件不为空则需要手动删除
```

执行成功后，需要在主节点按上述提示执行下面的命令

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

在各个子节点执行如下命令加入集群

```bash
sudo kubeadm join 192.168.56.12:6443 --token 2vdmjf.dpkodac4gz1oydtw \
	--discovery-token-ca-cert-hash  sha256:8d25f39ffe032f8c0e3456b7ea8861292ec799a059d96d17863b501161bb6bbd 
```

中间报错失败皆可以执行`sudo kubeadm reset`进行重置

子节点成功加入集群后的反馈信息：

```bash
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: missing optional cgroups: hugetlb
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

在控制节点查看全部节点信息：

```bash
app@K8sMaster:~$ kubectl get nodes
NAME        STATUS     ROLES                  AGE     VERSION
k8smaster   NotReady   control-plane,master   18m     v1.23.1
k8snode1    NotReady   <none>                 5m23s   v1.23.1
k8snode2    NotReady   <none>                 60s     v1.23.1

```

查看集群信息

```bash
app@K8sMaster:~$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.56.12:6443
CoreDNS is running at https://192.168.56.12:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

